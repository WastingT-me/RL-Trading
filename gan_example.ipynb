{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2VnKclJt6OXx"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from gan_model import GAN\n",
    "\n",
    "# Define generator parameters\n",
    "gen_kwargs = {\n",
    "    'noise_dim': 100,\n",
    "    'in_features': 50,\n",
    "    'cond_dim': 10,  # Only needed for Transformer generator\n",
    "    'out_features': 100,\n",
    "    'd_model': 256,  # Only needed for Transformer generator\n",
    "    'nhead': 4,  # Only needed for Transformer generator\n",
    "    'num_encoder_layers': 3,  # Only needed for Transformer generator\n",
    "    'num_decoder_layers': 3,  # Only needed for Transformer generator\n",
    "    'layer_sizes': [128, 256, 512],\n",
    "    'activation': 'relu',\n",
    "    'batchnorm': True,\n",
    "    'dropout_p': 0.2,\n",
    "    'nonnegative_end_ind': 50,\n",
    "    'use_skip_connections': False,\n",
    "    'add_vector': None,  # Only needed if you want to add a specific vector to the output\n",
    "}\n",
    "\n",
    "# Define discriminator parameters\n",
    "disc_kwargs = {\n",
    "    'in_features': 50,\n",
    "    'in_generated': 100,\n",
    "    'd_model': 256,  # Only needed for Transformer discriminator\n",
    "    'nhead': 4,  # Only needed for Transformer discriminator\n",
    "    'num_encoder_layers': 3,  # Only needed for Transformer discriminator\n",
    "    'num_decoder_layers': 3,  # Only needed for Transformer discriminator\n",
    "    'layer_sizes': [128, 256, 512],\n",
    "    'activation': 'relu',\n",
    "    'batchnorm': True,\n",
    "    'dropout_p': 0.2,\n",
    "    'use_skip_connections': False,\n",
    "}\n",
    "\n",
    "# Instantiate the GAN model\n",
    "gan = GAN(\n",
    "    gen_arch='MLP',  # or 'Transformer'\n",
    "    disc_arch='Transformer',  # or 'MLP'\n",
    "    gen_kwargs=gen_kwargs,\n",
    "    disc_kwargs=disc_kwargs\n",
    ")\n",
    "\n",
    "# Dummy data for training\n",
    "batch_size = 16\n",
    "noise = gan.generator.get_noise(batch_size)\n",
    "real_data = torch.randn(batch_size, gen_kwargs['in_features'])\n",
    "cond_data = torch.randn(batch_size, gen_kwargs['cond_dim'])  # Only needed for Transformer generator\n",
    "\n",
    "# Forward pass through GAN\n",
    "fake_data, disc_real, disc_fake = gan(noise, real_data, cond_data)\n",
    "\n",
    "optimizer_G = torch.optim.Adam(gan.generator.parameters(), lr=0.0002)\n",
    "optimizer_D = torch.optim.Adam(gan.discriminator.parameters(), lr=0.0002)\n",
    "\n",
    "for epoch in range(1):\n",
    "    # Train Discriminator\n",
    "    optimizer_D.zero_grad()\n",
    "    _, disc_real, disc_fake = gan(noise, real_data, cond_data)\n",
    "    loss_D = -(torch.log(disc_real) + torch.log(1 - disc_fake)).mean()\n",
    "    loss_D.backward()\n",
    "    optimizer_D.step()\n",
    "\n",
    "    # Train Generator\n",
    "    optimizer_G.zero_grad()\n",
    "    fake_data, _, disc_fake = gan(noise, real_data, cond_data)\n",
    "    loss_G = -torch.log(disc_fake).mean()\n",
    "    loss_G.backward()\n",
    "    optimizer_G.step()\n",
    "\n",
    "    print(f\"Epoch {epoch}: Loss D: {loss_D.item()}, Loss G: {loss_G.item()}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
